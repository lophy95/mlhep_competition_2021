{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-893409db-7752-488f-b38c-ab955c05345e.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1627559239525,"exec_count":1,"id":"307bbf","input":"import configparser\nimport pathlib as path\nimport pytorch_lightning as pl\nfrom pytorch_lightning import seed_everything\nfrom idao.data_module import IDAODataModule\nfrom idao.model import SimpleConv\nimport numpy as np\nimport torch\nimport os\nimport pathlib as path\nfrom PIL import Image\n\nfrom torchvision.datasets import DatasetFolder\nfrom torch.utils.data import Dataset\nimport pathlib as path\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport configparser\nimport pathlib as path\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import seed_everything\n\nimport configparser\nimport gc\nimport logging\nimport pathlib as path\nimport sys\nfrom collections import defaultdict\nfrom itertools import chain\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scikitplot as skplt\nimport torch\nfrom more_itertools import bucket\n\n#from idao.data_module import IDAODataModule\n#from idao.model import SimpleConv\nfrom idao.utils import delong_roc_variance\n","kernel":"python3","pos":0,"start":1627559236956,"state":"done","type":"cell"}
{"cell_type":"code","end":1627559239544,"exec_count":2,"id":"038860","input":"class IDAODataset(DatasetFolder):\n    def name_to_energy(self, name):\n        names = os.path.split(name)[-1].split(\"_\")\n        idx = [i for i, v in enumerate(names) if v == \"keV\"][0]\n        return torch.tensor(float(names[idx - 1]))\n\n    def name_to_index(self, name):\n        return os.path.split(name)[-1].split('.')[0]\n\n    def __getitem__(self, index: int):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target, self.name_to_energy(path), self.name_to_index(path)\n\nclass InferenceDataset(Dataset):\n    def __init__(self, main_dir, transform, loader=None):\n        self.img_loaderj= img_loader\n        self.main_dir = path.Path(main_dir)\n        self.transform = transform\n        self.all_imgs = list(self.main_dir.glob(\"*.png\"))\n        self.loader = loader\n\n    def __len__(self):\n        return len(self.all_imgs)\n\n    def __getitem__(self, idx):\n        img_loc = self.all_imgs[idx]\n        image = self.loader(img_loc)\n        tensor_image = self.transform(image)\n        return tensor_image, img_loc.name\n\ndef img_loader(path: str):\n    with Image.open(path) as img:\n        img = np.array(img)\n    return img\n","kernel":"python3","pos":1,"start":1627559239541,"state":"done","type":"cell"}
{"cell_type":"code","end":1627559239559,"exec_count":3,"id":"e2c369","input":"class IDAODataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: path.Path, batch_size: int, cfg):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.cfg = cfg\n\n    def prepare_data(self):\n        # called only on 1 GPU\n        self.dataset = IDAODataset(\n            root=self.data_dir.joinpath(\"train\"),\n            loader=img_loader,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.CenterCrop(120)]  #CenterCrop(160)\n            ),\n            # TODO(kazeevn) use idiomatic torch\n            target_transform=transforms.Compose(\n                [\n                    lambda num: (\n                        torch.tensor([0, 1]) if num == 0 else torch.tensor([1, 0])\n                    )\n                ]\n            ),\n            extensions=self.cfg[\"DATA\"][\"Extension\"],\n        )\n\n        self.test = InferenceDataset(\n                    main_dir=self.data_dir.joinpath(\"test\"),\n                    loader=img_loader,\n                    transform=transforms.Compose(\n                        [transforms.ToTensor(), transforms.CenterCrop(120)]#CenterCrop(160)\n                    ),\n                )\n\n\n    def setup(self, stage=None):\n        # called on every GPU\n        self.train, self.val = random_split(\n            self.dataset, [10000, 3404], generator=torch.Generator().manual_seed(666)\n        )\n\n    def train_dataloader(self):\n        return DataLoader(self.train, self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.val, 1, num_workers=1, shuffle=False)\n    \n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test,\n            self.batch_size,\n            num_workers=0,\n            shuffle=False\n            )\n\n","kernel":"python3","pos":2,"start":1627559239556,"state":"done","type":"cell"}
{"cell_type":"code","end":1627559239580,"exec_count":4,"id":"28bb41","input":"class Print(nn.Module):\n    \"\"\"Debugging only\"\"\"\n\n    def forward(self, x):\n        print(x.size())\n        return x\n\n\nclass Clamp(nn.Module):\n    \"\"\"Clamp energy output\"\"\"\n\n    def forward(self, x):\n        x = torch.clamp(x, min=0, max=30)\n        return x\n\n\nclass SimpleConv(pl.LightningModule):\n    def __init__(self, mode: [\"classification\", \"regression\"] = \"classification\"):\n        super().__init__()\n        self.mode = mode\n        self.layer1 = nn.Sequential(\n                    nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n                    nn.BatchNorm2d(16),\n                    nn.ReLU(),\n                    nn.MaxPool2d(6),\n                    nn.Conv2d(16,32,4,stride=1,padding=1),\n                    nn.BatchNorm2d(32),\n                    nn.ReLU(),\n                    nn.MaxPool2d(kernel_size=6,stride=3),\n                    nn.Flatten(),\n                )\n        \n\n        self.drop_out = nn.Dropout(p=0.4)\n\n        self.fc1 = nn.Linear(800, 200)\n        self.fc2 = nn.Linear(200, 2)  # for classification\n        self.fc3 = nn.Linear(200, 1)  # for regression\n\n        self.stem = nn.Sequential(\n            self.layer1, self.drop_out, self.fc1,\n            )\n        if self.mode == \"classification\":\n            self.classification = nn.Sequential(self.stem, self.fc2)\n        else:\n            self.regression = nn.Sequential(self.stem, self.fc3)\n\n        self.train_acc = pl.metrics.Accuracy()\n        self.valid_acc = pl.metrics.Accuracy()\n        self.test_acc = pl.metrics.Accuracy()\n\n    def training_step(self, batch, batch_idx):\n        # --------------------------\n        x_target, class_target, reg_target, _ = batch\n        if self.mode == \"classification\":\n            class_pred = self.classification(x_target.float())\n            class_loss = F.binary_cross_entropy_with_logits(\n                class_pred, class_target.float()\n            )\n            self.train_acc(torch.sigmoid(class_pred), class_target)\n            self.log(\"train_acc\", self.train_acc, on_step=True, on_epoch=False)\n            self.log(\"classification_loss\", class_loss)\n\n            return class_loss\n\n        else:\n            reg_pred = self.regression(x_target.float())\n            #             reg_loss = F.l1_loss(reg_pred, reg_target.float().view(-1, 1))\n            reg_loss = F.mse_loss(reg_pred, reg_target.float().view(-1, 1))\n\n            #             reg_loss = torch.sum(torch.abs(reg_pred - reg_target.float().view(-1, 1)) / reg_target.float().view(-1, 1))\n            self.log(\"regression_loss\", reg_loss)\n            return reg_loss\n\n    def training_epoch_end(self, outs):\n        # log epoch metric\n        if self.mode == \"classification\":\n            self.log(\"train_acc_epoch\", self.train_acc.compute())\n        else:\n            pass\n\n    def validation_step(self, batch, batch_idx):\n        x_target, class_target, reg_target, _ = batch\n        if self.mode == \"classification\":\n            class_pred = self.classification(x_target.float())\n            class_loss = F.binary_cross_entropy_with_logits(\n                class_pred, class_target.float()\n            )\n            self.valid_acc(torch.sigmoid(class_pred), class_target)\n            self.log(\"valid_acc\", self.valid_acc.compute())\n            self.log(\"classification_loss\", class_loss)\n            return class_loss\n\n        else:\n            reg_pred = self.regression(x_target.float())\n            #             reg_loss = F.l1_loss(reg_pred, reg_target.float().view(-1, 1))\n            reg_loss = F.mse_loss(reg_pred, reg_target.float().view(-1, 1))\n\n            #             reg_loss = torch.sum(torch.abs(reg_pred - reg_target.float().view(-1, 1)) / reg_target.float().view(-1, 1))\n            self.log(\"regression_loss\", reg_loss)\n            return reg_loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adamax(self.parameters(), lr=1e-3)\n        return optimizer\n\n    def forward(self, x):\n        if self.mode == \"classification\":\n            class_pred = self.classification(x.float())\n            return {\"class\": torch.sigmoid(class_pred)}\n        else:\n            reg_pred = self.regression(x.float())\n            return {\"energy\": reg_pred}\n","kernel":"python3","pos":3,"start":1627559239577,"state":"done","type":"cell"}
{"cell_type":"code","end":1627559239601,"exec_count":5,"id":"c68dca","input":"def test_variance(target, predictions):\n    return torch.std(torch.abs(predictions - target) / target) ** 2\n\n\ndef run_test(mode, dataloader, checkpoint_path, cfg):\n    torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    logging.info(\"Loading checkpoint\")\n    model = SimpleConv.load_from_checkpoint(checkpoint_path, mode=mode)\n    model = model.cpu().eval()\n    regression_predictions = []\n    classification_predictions = []\n    classification_target = []\n    regression_target = []\n\n    if mode == \"classification\":\n        logging.info(\"Classification model loaded\")\n    else:\n        logging.info(\"Regression model loaded\")\n\n    for i, (img, class_label, regression_label, _) in enumerate(iter(dataloader)):\n        if mode == \"classification\":\n            output = model(img)[\"class\"].detach()\n            classification_predictions.append(output)\n            classification_target.append(class_label)\n        else:\n            output = model(img)[\"energy\"].detach()\n            regression_predictions.append(output)\n            regression_target.append(regression_label)\n\n    #        del output\n\n    if mode == \"classification\":\n        logging.info(\"Starting classification task\")\n        classification_predictions = torch.cat(classification_predictions, dim=0)\n        classification_target = torch.cat(classification_target, dim=0)\n        new_target = np.argmax(classification_target.detach().cpu().numpy(), axis=1)\n        auc, variance = delong_roc_variance(\n            new_target, classification_predictions.detach().cpu().numpy()[:, 1]\n        )\n        skplt.metrics.plot_roc(\n            classification_target.max(1).indices,\n            classification_predictions.detach().cpu().numpy(),\n            plot_macro=False,\n            plot_micro=False,\n            classes_to_plot=[0]\n        )\n        plt.savefig(f'{cfg[\"REPORT\"][\"SaveDir\"]}/roc_auc.png', dpi=196)\n        plt.show()\n        logging.info(f'ROC plot saved at: {cfg[\"REPORT\"][\"SaveDir\"]}/roc_auc.png')\n        logging.info(f\"Delong => ROC-AUC: {auc} variance: {variance}\")\n\n        del classification_predictions\n        del classification_target\n        # gc.collect() # Invoke the garbage collector\n        return (None, auc)\n\n    else:\n        logging.info(\"Starting regression task\")\n        regression_predictions = torch.tensor(\n            list(chain(*regression_predictions))\n        ).view(-1)\n        regression_target = torch.tensor(list(chain(*regression_target))).view(-1)\n        variance = test_variance(regression_target, regression_predictions)\n        #plt.savefig(f'{cfg[\"REPORT\"][\"SaveDir\"]}/roc_auc.png', dpi=196)\n        #plt.show()\n        logging.info(f\"Test energy variance: {variance}\")\n\n        # MAE\n        mae = torch.nn.functional.l1_loss(regression_predictions, regression_target)\n\n        # plot correlation\n        fig, ax = plt.subplots()\n        ax.plot(\n            regression_target, regression_predictions, \"ro\", label=\"Energy Prediction\"\n        )\n        ax.set_xlabel(\"True Energy\")\n        ax.set_ylabel(\"Predicted Energy\")\n        ax.legend()\n        ax.grid()\n        fig.savefig(f'{cfg[\"REPORT\"][\"SaveDir\"]}/energy_correlation.png', dpi=196)\n        logging.info(f'Energy correlation plot saved at: {cfg[\"REPORT\"][\"SaveDir\"]}/energy_correlation.png')\n        plt.show(fig)\n        plt.close(fig)\n\n        logging.info(f'===> Length: {len(regression_predictions)} {len(regression_target)}')\n        # plot comparison\n        fig1, ax1 = plt.subplots()\n        ax1.plot(regression_predictions, \"bo\", alpha=0.6, label=\"Predicted Energy\")\n        ax1.plot(regression_target, \"ro\", label=\"True Energy\")\n        ax1.legend()\n        ax1.grid()\n        fig1.savefig(f'{cfg[\"REPORT\"][\"SaveDir\"]}/energy_comparison.png', dpi=196)\n        logging.info(f'Energy comparison plot saved at: {cfg[\"REPORT\"][\"SaveDir\"]}/energy_comparison.png')\n        plt.show(fig1)\n        plt.close(fig1)\n\n        # plot histograms\n        group = zip(regression_target, regression_predictions)\n        group = sorted(group, key=lambda item: item[0])\n        logging.info(regression_target)\n\n        data_dict = defaultdict(list)\n\n        for t, p in group:\n            data_dict[t.item()].append(p.item())\n\n        for i, (k, v) in enumerate(data_dict.items()):\n            fig3, ax3 = plt.subplots()\n            ax3.hist(\n                v,\n                bins=100,\n                histtype=\"step\",\n                label=f\"Energy: {k} keV \\n RMS: {float(torch.sqrt(torch.mean(torch.tensor(v)**2))):.03f} \\n Mean: {float(torch.mean(torch.tensor(v))):.03f}\",\n            )\n            ax3.legend()\n            fig3.savefig(f'{cfg[\"REPORT\"][\"SaveDir\"]}/energy_hist{k}_{i}.png', dpi=196)\n            plt.close(fig3)\n\n            logging.info(\n                f'Histogram {k} keV saved at: {cfg[\"REPORT\"][\"SaveDir\"]}/energy_hist{k}_{i}.png'\n            )\n\n        return (mae, None)\n","kernel":"python3","pos":4,"start":1627559239599,"state":"done","type":"cell"}
{"cell_type":"code","end":1627559239610,"exec_count":6,"id":"7d28a7","input":"def main(cfg):\n    PATH = path.Path(cfg[\"DATA\"][\"DatasetPath\"])\n\n    dataset_dm = IDAODataModule(\n        data_dir=PATH, batch_size=64, cfg=cfg\n    )\n\n    dataset_dm.prepare_data()\n    dataset_dm.setup()\n    dl = dataset_dm.train_dataloader()\n    mae = 0\n    variance = 0\n\n    for mode in [\"regression\", \"classification\"]:\n        if mode == \"classification\":\n            model_path = cfg[\"REPORT\"][\"ClassificationCheckpoint\"]\n        else:\n            model_path = cfg[\"REPORT\"][\"RegressionCheckpoint\"]\n\n        _mae, _auc = run_test(mode, dl, model_path, cfg=cfg)\n        if _mae is not None:\n            mae = _mae\n        if _auc is not None:\n            auc = _auc\n\n\n        gc.collect()\n    logging.info(f'MAE = {mae}')\n    logging.info(f'AUC = {auc}')\n\n","kernel":"python3","pos":5,"start":1627559239607,"state":"done","type":"cell"}
{"cell_type":"code","end":1627559437727,"exec_count":7,"id":"a76761","input":"if __name__ == \"__main__\":\n    config = configparser.ConfigParser()\n    config.read(\"./config.ini\")\n\n    logging.basicConfig(\n        level=logging.INFO,\n        handlers=[\n            logging.FileHandler(f'{config[\"REPORT\"][\"SaveDir\"]}report.log'),\n            logging.StreamHandler(sys.stdout),\n        ],\n    )\n    main(cfg=config)","kernel":"python3","output":{"0":{"name":"stdout","text":"INFO:root:Loading checkpoint\n"},"1":{"name":"stdout","text":"INFO:root:Regression model loaded\n"},"10":{"data":{"image/png":"d420fc047f64579edc664df8632d9477aada8ec6","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}},"11":{"name":"stdout","text":"INFO:root:tensor([20., 20., 10.,  ..., 30.,  3., 10.])\n"},"12":{"name":"stdout","text":"INFO:root:Histogram 1.0 keV saved at: ./results/energy_hist1.0_0.png\n"},"13":{"name":"stdout","text":"INFO:root:Histogram 3.0 keV saved at: ./results/energy_hist3.0_1.png\n"},"14":{"name":"stdout","text":"INFO:root:Histogram 6.0 keV saved at: ./results/energy_hist6.0_2.png\n"},"15":{"name":"stdout","text":"INFO:root:Histogram 10.0 keV saved at: ./results/energy_hist10.0_3.png\n"},"16":{"name":"stdout","text":"INFO:root:Histogram 20.0 keV saved at: ./results/energy_hist20.0_4.png\n"},"17":{"name":"stdout","text":"INFO:root:Histogram 30.0 keV saved at: ./results/energy_hist30.0_5.png\n"},"18":{"name":"stdout","text":"INFO:root:Loading checkpoint"},"19":{"name":"stdout","text":"\n"},"2":{"name":"stderr","text":"/usr/local/lib/python3.6/dist-packages/deprecate/deprecation.py:115: LightningDeprecationWarning:\n\nThe `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n\n"},"20":{"name":"stdout","text":"INFO:root:Classification model loaded\n"},"21":{"name":"stdout","text":"INFO:root:Starting classification task\n"},"22":{"data":{"image/png":"0562bf3fdc4f190aae201da524bda003e98da124","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}},"23":{"name":"stdout","text":"INFO:root:ROC plot saved at: ./results/roc_auc.png\n"},"24":{"name":"stdout","text":"INFO:root:Delong => ROC-AUC: 0.9988354291488191 variance: 2.5425233487206194e-08\n"},"25":{"name":"stdout","text":"INFO:root:MAE = 1.0484306812286377\n"},"26":{"name":"stdout","text":"INFO:root:AUC = 0.9988354291488191\n"},"3":{"name":"stderr","text":"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning:\n\nNamed tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n\n"},"4":{"name":"stdout","text":"INFO:root:Starting regression task\n"},"5":{"name":"stdout","text":"INFO:root:Test energy variance: 0.07867857813835144\n"},"6":{"name":"stdout","text":"INFO:root:Energy correlation plot saved at: ./results/energy_correlation.png\n"},"7":{"data":{"image/png":"f1d2895c05d40e3166ec8f1cebef29e83505285c","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}},"8":{"name":"stdout","text":"INFO:root:===> Length: 10000 10000\n"},"9":{"name":"stdout","text":"INFO:root:Energy comparison plot saved at: ./results/energy_comparison.png\n"}},"pos":6,"start":1627559239614,"state":"done","type":"cell"}
{"cell_type":"code","id":"385315","input":"","pos":7,"type":"cell"}
{"id":0,"time":1627559061075,"type":"user"}
{"last_load":1627559059789,"type":"file"}