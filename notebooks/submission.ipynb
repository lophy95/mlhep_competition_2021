{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import configparser\n",
    "import gc\n",
    "import logging\n",
    "import pathlib as path\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import configparser\n",
    "import pathlib as path\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from idao.data_module import IDAODataModule\n",
    "from idao.model import SimpleConv\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pathlib as path\n",
    "from PIL import Image\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import Dataset\n",
    "import pathlib as path\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import configparser\n",
    "import pathlib as path\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import configparser\n",
    "import gc\n",
    "import logging\n",
    "import pathlib as path\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import scikitplot as skplt\n",
    "import torch\n",
    "from more_itertools import bucket\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#from idao.data_module import IDAODataModule\n",
    "#from idao.model import SimpleConv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class IDAODataset(DatasetFolder):\n",
    "    def name_to_energy(self, name):\n",
    "        names = os.path.split(name)[-1].split(\"_\")\n",
    "        idx = [i for i, v in enumerate(names) if v == \"keV\"][0]\n",
    "        return torch.tensor(float(names[idx - 1]))\n",
    "\n",
    "    def name_to_index(self, name):\n",
    "        return os.path.split(name)[-1].split('.')[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target, self.name_to_energy(path), self.name_to_index(path)\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, main_dir, transform, loader=None):\n",
    "        self.img_loaderj= img_loader\n",
    "        self.main_dir = path.Path(main_dir)\n",
    "        self.transform = transform\n",
    "        self.all_imgs = list(self.main_dir.glob(\"*.png\"))\n",
    "        self.loader = loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = self.all_imgs[idx]\n",
    "        image = self.loader(img_loc)\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image, img_loc.name\n",
    "\n",
    "def img_loader(path: str):\n",
    "    with Image.open(path) as img:\n",
    "        img = np.array(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class IDAODataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: path.Path, batch_size: int, cfg):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # called only on 1 GPU\n",
    "        self.dataset = IDAODataset(\n",
    "            root=self.data_dir.joinpath(\"train\"),\n",
    "            loader=img_loader,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.CenterCrop(120)]  #CenterCrop(120)\n",
    "            ),\n",
    "            # TODO(kazeevn) use idiomatic torch\n",
    "            target_transform=transforms.Compose(\n",
    "                [\n",
    "                    lambda num: (\n",
    "                        torch.tensor([0, 1]) if num == 0 else torch.tensor([1, 0])\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            extensions=self.cfg[\"DATA\"][\"Extension\"],\n",
    "        )\n",
    "\n",
    "        self.test = InferenceDataset(\n",
    "                    main_dir=self.data_dir.joinpath(\"test\"),\n",
    "                    loader=img_loader,\n",
    "                    transform=transforms.Compose(\n",
    "                        [transforms.ToTensor(), transforms.CenterCrop(120)]#CenterCrop(120)\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # called on every GPU\n",
    "        self.train, self.val = random_split(\n",
    "            self.dataset, [10000, 3404], generator=torch.Generator().manual_seed(666)\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, 1, num_workers=1, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class Print(nn.Module):\n",
    "    \"\"\"Debugging only\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        return x\n",
    "\n",
    "\n",
    "class Clamp(nn.Module):\n",
    "    \"\"\"Clamp energy output\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x, min=0, max=30)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleConv(pl.LightningModule):\n",
    "    def __init__(self, mode: [\"classification\", \"regression\"] = \"classification\"):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.layer1 = nn.Sequential(\n",
    "                    nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "                    nn.BatchNorm2d(16),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(6),\n",
    "                    nn.Conv2d(16,32,4,stride=1,padding=1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=6,stride=3),\n",
    "                    nn.Flatten(),\n",
    "                )\n",
    "        \n",
    "\n",
    "        self.drop_out = nn.Dropout()\n",
    "\n",
    "        self.fc1 = nn.Linear(800, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)  # for classification\n",
    "        self.fc3 = nn.Linear(100, 1)  # for regression\n",
    "\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            self.layer1, self.drop_out, self.fc1,\n",
    "            )\n",
    "        if self.mode == \"classification\":\n",
    "            self.classification = nn.Sequential(self.stem, self.fc2)\n",
    "        else:\n",
    "            self.regression = nn.Sequential(self.stem, self.fc3)\n",
    "\n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.valid_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # --------------------------\n",
    "        x_target, class_target, reg_target, _ = batch\n",
    "        if self.mode == \"classification\":\n",
    "            class_pred = self.classification(x_target.float())\n",
    "            class_loss = F.binary_cross_entropy_with_logits(\n",
    "                class_pred, class_target.float()\n",
    "            )\n",
    "            self.train_acc(torch.sigmoid(class_pred), class_target)\n",
    "            self.log(\"train_acc\", self.train_acc, on_step=True, on_epoch=False)\n",
    "            self.log(\"classification_loss\", class_loss)\n",
    "\n",
    "            return class_loss\n",
    "\n",
    "        else:\n",
    "            reg_pred = self.regression(x_target.float())\n",
    "            #             reg_loss = F.l1_loss(reg_pred, reg_target.float().view(-1, 1))\n",
    "            reg_loss = F.mse_loss(reg_pred, reg_target.float().view(-1, 1))\n",
    "\n",
    "            #             reg_loss = torch.sum(torch.abs(reg_pred - reg_target.float().view(-1, 1)) / reg_target.float().view(-1, 1))\n",
    "            self.log(\"regression_loss\", reg_loss)\n",
    "            return reg_loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        if self.mode == \"classification\":\n",
    "            self.log(\"train_acc_epoch\", self.train_acc.compute())\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_target, class_target, reg_target, _ = batch\n",
    "        if self.mode == \"classification\":\n",
    "            class_pred = self.classification(x_target.float())\n",
    "            class_loss = F.binary_cross_entropy_with_logits(\n",
    "                class_pred, class_target.float()\n",
    "            )\n",
    "            self.valid_acc(torch.sigmoid(class_pred), class_target)\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.log(\"classification_loss\", class_loss)\n",
    "            return class_loss\n",
    "\n",
    "        else:\n",
    "            reg_pred = self.regression(x_target.float())\n",
    "            #             reg_loss = F.l1_loss(reg_pred, reg_target.float().view(-1, 1))\n",
    "            reg_loss = F.mse_loss(reg_pred, reg_target.float().view(-1, 1))\n",
    "\n",
    "            #             reg_loss = torch.sum(torch.abs(reg_pred - reg_target.float().view(-1, 1)) / reg_target.float().view(-1, 1))\n",
    "            self.log(\"regression_loss\", reg_loss)\n",
    "            return reg_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == \"classification\":\n",
    "            class_pred = self.classification(x.float())\n",
    "            return {\"class\": torch.sigmoid(class_pred)}\n",
    "        else:\n",
    "            reg_pred = self.regression(x.float())\n",
    "            return {\"energy\": reg_pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def compute_predictions(mode, dataloader, checkpoint_path, cfg):\n",
    "    torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "    logging.info(\"Loading checkpoint\")\n",
    "    model = SimpleConv.load_from_checkpoint(checkpoint_path, mode=mode)\n",
    "    model = model.cpu().eval()\n",
    "\n",
    "    dict_pred = defaultdict(list)\n",
    "    if mode == \"classification\":\n",
    "        logging.info(\"Classification model loaded\")\n",
    "    else:\n",
    "        logging.info(\"Regression model loaded\")\n",
    "\n",
    "    for img, name in iter(dataloader):\n",
    "        if mode == \"classification\":\n",
    "            dict_pred[\"id\"].extend(map(lambda x: x.strip('.png'), name))\n",
    "            output = model(img)[\"class\"].detach()[:, 1].numpy()\n",
    "            dict_pred[\"particle\"].extend(output)\n",
    "        else:\n",
    "            output = model(img)[\"energy\"].detach().squeeze(1).numpy()\n",
    "            dict_pred[\"energy\"].extend(output)\n",
    "            \n",
    "    return dict_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"./config.ini\")\n",
    "    PATH = path.Path(config[\"DATA\"][\"DatasetPath\"])\n",
    "\n",
    "    dataset_dm = IDAODataModule(\n",
    "        data_dir=PATH, batch_size=512, cfg=config\n",
    "    )\n",
    "\n",
    "    dataset_dm.prepare_data()\n",
    "    print(dataset_dm.dataset.class_to_idx)\n",
    "    dataset_dm.setup()\n",
    "    dl = dataset_dm.test_dataloader()\n",
    "\n",
    "    dict_pred = defaultdict(list)\n",
    "    for mode in [\"regression\", \"classification\"]:\n",
    "        if mode == \"classification\":\n",
    "            model_path = config[\"REPORT\"][\"ClassificationCheckpoint\"]\n",
    "        else:\n",
    "            model_path = config[\"REPORT\"][\"RegressionCheckpoint\"]\n",
    "\n",
    "        dict_pred.update(compute_predictions(mode, dl, model_path, cfg=config))\n",
    "\n",
    "    data_frame = pd.DataFrame(dict_pred,\n",
    "                              columns=[\"id\", \"energy\", \"particle\"])\n",
    "    data_frame.set_index(\"id\", inplace=True)\n",
    "    data_frame.to_csv('submission_classification_T36.csv.gz',\n",
    "                      index=True, header=True, index_label=\"id\", columns=[\"particle\"])\n",
    "    data_frame.to_csv('submission_regression_T36.csv.gz',\n",
    "                      index=True, header=True, index_label=\"id\", columns=[\"energy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ER': 0, 'NR': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/deprecate/deprecation.py:115: LightningDeprecationWarning:\n",
      "\n",
      "The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning:\n",
      "\n",
      "Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}